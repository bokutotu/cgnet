{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse Graining Alanine Dipeptide: Force Matching via CGnet\n",
    "\n",
    "In this tutorial, we use CGnet [1] to perform a force-matching analysis of alanine dipeptide in order to produce a coarse-grained force field. While this replicates the analysis in the original publication, note that the alanine dipeptide dataset is truncated and the tutorial therefore does _not_ represent a state of the art analysis; rather, it is designed to avoid time-consuming calculations and is an illustration of the basic pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and loading data\n",
    "\n",
    "First, we import all the necessary packages. Make sure that you've installed cgnet and all of its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from cgnet.feature import (GeometryFeature, GeometryStatistics,\n",
    "                           MoleculeDataset, LinearLayer)\n",
    "from cgnet.network import (CGnet, HarmonicLayer, ForceLoss, ZscoreLayer,\n",
    "                           lipschitz_projection, dataset_loss, Simulation)\n",
    "\n",
    "import mdtraj as md\n",
    "from cgnet.molecule import CGMolecule\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# We specify the CPU as the training/simulating device here.\n",
    "# If you have machine  with a GPU, you can use the GPU for\n",
    "# accelerated training/simulation by specifying \n",
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load our example dataset. The 10,000 data points are spaced at 10 picosecond intervals for a total of 100 nanoseconds of simulation time (compare with the CGnet paper [1], in which the same system is analyzed with a 1 microsecond trajectory at 1 picosecond intervals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.load('../../python_preprocess/removed_c_trj.npy')\n",
    "forces = np.load('../../python_preprocess/removed_f_trj.npy')\n",
    "\n",
    "coords = coords.astype(np.float32)\n",
    "forces = forces.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 10,000 data points has the three-dimensional coordinates and forces of each of the five coarse-grained beads (the five backbone atoms). We verify that the dimensions of the coordinates and forces are (number of frames, number of beads, number of dimensions) by printing their shapes.\n",
    "\n",
    "Given correctly-shaped coordinates and corresponding forces, we can make a `MoleculeDataset` that will interface with our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates size: (999496, 60, 3)\n",
      "Force: (999496, 60, 3)\n",
      "Dataset length: 999496\n"
     ]
    }
   ],
   "source": [
    "print(\"Coordinates size: {}\".format(coords.shape))\n",
    "print(\"Force: {}\".format(forces.shape))\n",
    "\n",
    "ala_data = MoleculeDataset(coords, forces, device=device)\n",
    "print(\"Dataset length: {}\".format(len(ala_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering statistics\n",
    "\n",
    "The harmonic contributions to the free energy from bonds and angles provide an important regularizing prior form on the energy learned by CGnet. Thus, we need to gather some statistics about our data's features. Specifically, we need the `get_bond_constants` method in the `GeometryStatistics` class.\n",
    "\n",
    "First, we gather statistics specifically for bonds and angles, respectively. Using the `as_list` argument, we can get lists of statistics dictionaries that can be used to construct important prior energies for our CGnet model.\n",
    "\n",
    "Note that _lists_ contain values, _keys_ contain tuples that specify a feature (e.g., a distance or an angle), and _indices_ contain integers used to locate certain features within _list_ or _key_ objects.\n",
    "\n",
    "If we look at the dictionaries contained inside the _list_ objects, we see that the means, standard deviations, and harmonic constants are stored according to a tuple containing the bead indices (not the same as the _indices_ above!) of the involved atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 60 backbone beads, 59 bonds, and 58 angles.\n",
      "Bonds: \n",
      "(0, 1) : {'mean': tensor(0.1489), 'std': tensor(0.0035), 'k': tensor(50077.9336)}\n",
      "(1, 2) : {'mean': tensor(0.1524), 'std': tensor(0.0034), 'k': tensor(52318.1953)}\n",
      "(2, 3) : {'mean': tensor(0.1350), 'std': tensor(0.0028), 'k': tensor(76645.3203)}\n",
      "(3, 4) : {'mean': tensor(0.1465), 'std': tensor(0.0029), 'k': tensor(69041.3516)}\n",
      "(4, 5) : {'mean': tensor(0.1518), 'std': tensor(0.0034), 'k': tensor(52034.0078)}\n",
      "(5, 6) : {'mean': tensor(0.1347), 'std': tensor(0.0028), 'k': tensor(77727.4766)}\n",
      "(6, 7) : {'mean': tensor(0.1463), 'std': tensor(0.0030), 'k': tensor(66964.9219)}\n",
      "(7, 8) : {'mean': tensor(0.1521), 'std': tensor(0.0034), 'k': tensor(52452.0352)}\n",
      "(8, 9) : {'mean': tensor(0.1350), 'std': tensor(0.0028), 'k': tensor(74603.5703)}\n",
      "(9, 10) : {'mean': tensor(0.1453), 'std': tensor(0.0030), 'k': tensor(67126.0469)}\n",
      "(10, 11) : {'mean': tensor(0.1531), 'std': tensor(0.0034), 'k': tensor(52223.5781)}\n",
      "(11, 12) : {'mean': tensor(0.1351), 'std': tensor(0.0028), 'k': tensor(73998.5312)}\n",
      "(12, 13) : {'mean': tensor(0.1454), 'std': tensor(0.0030), 'k': tensor(68496.1250)}\n",
      "(13, 14) : {'mean': tensor(0.1529), 'std': tensor(0.0034), 'k': tensor(51819.5117)}\n",
      "(14, 15) : {'mean': tensor(0.1353), 'std': tensor(0.0028), 'k': tensor(77161.9766)}\n",
      "(15, 16) : {'mean': tensor(0.1457), 'std': tensor(0.0030), 'k': tensor(67048.7578)}\n",
      "(16, 17) : {'mean': tensor(0.1528), 'std': tensor(0.0033), 'k': tensor(53469.7266)}\n",
      "(17, 18) : {'mean': tensor(0.1350), 'std': tensor(0.0028), 'k': tensor(75288.0703)}\n",
      "(18, 19) : {'mean': tensor(0.1454), 'std': tensor(0.0030), 'k': tensor(67906.2656)}\n",
      "(19, 20) : {'mean': tensor(0.1528), 'std': tensor(0.0034), 'k': tensor(51730.5859)}\n",
      "(20, 21) : {'mean': tensor(0.1351), 'std': tensor(0.0028), 'k': tensor(74008.0312)}\n",
      "(21, 22) : {'mean': tensor(0.1456), 'std': tensor(0.0030), 'k': tensor(67125.4609)}\n",
      "(22, 23) : {'mean': tensor(0.1527), 'std': tensor(0.0034), 'k': tensor(52671.7695)}\n",
      "(23, 24) : {'mean': tensor(0.1351), 'std': tensor(0.0028), 'k': tensor(76567.6875)}\n",
      "(24, 25) : {'mean': tensor(0.1457), 'std': tensor(0.0030), 'k': tensor(66697.8125)}\n",
      "(25, 26) : {'mean': tensor(0.1525), 'std': tensor(0.0034), 'k': tensor(51796.0938)}\n",
      "(26, 27) : {'mean': tensor(0.1350), 'std': tensor(0.0028), 'k': tensor(75329.6094)}\n",
      "(27, 28) : {'mean': tensor(0.1451), 'std': tensor(0.0030), 'k': tensor(65322.3281)}\n",
      "(28, 29) : {'mean': tensor(0.1528), 'std': tensor(0.0034), 'k': tensor(51478.2969)}\n",
      "(29, 30) : {'mean': tensor(0.1352), 'std': tensor(0.0028), 'k': tensor(75376.9922)}\n",
      "(30, 31) : {'mean': tensor(0.1462), 'std': tensor(0.0030), 'k': tensor(68096.7109)}\n",
      "(31, 32) : {'mean': tensor(0.1528), 'std': tensor(0.0034), 'k': tensor(52734.0391)}\n",
      "(32, 33) : {'mean': tensor(0.1351), 'std': tensor(0.0028), 'k': tensor(75838.9844)}\n",
      "(33, 34) : {'mean': tensor(0.1457), 'std': tensor(0.0030), 'k': tensor(66297.4531)}\n",
      "(34, 35) : {'mean': tensor(0.1529), 'std': tensor(0.0034), 'k': tensor(52854.7539)}\n",
      "(35, 36) : {'mean': tensor(0.1351), 'std': tensor(0.0028), 'k': tensor(75776.8906)}\n",
      "(36, 37) : {'mean': tensor(0.1457), 'std': tensor(0.0029), 'k': tensor(70101.8906)}\n",
      "(37, 38) : {'mean': tensor(0.1528), 'std': tensor(0.0034), 'k': tensor(51837.5508)}\n",
      "(38, 39) : {'mean': tensor(0.1350), 'std': tensor(0.0028), 'k': tensor(76507.1875)}\n",
      "(39, 40) : {'mean': tensor(0.1462), 'std': tensor(0.0030), 'k': tensor(66410.4531)}\n",
      "(40, 41) : {'mean': tensor(0.1531), 'std': tensor(0.0033), 'k': tensor(53188.7422)}\n",
      "(41, 42) : {'mean': tensor(0.1351), 'std': tensor(0.0028), 'k': tensor(74699.7891)}\n",
      "(42, 43) : {'mean': tensor(0.1455), 'std': tensor(0.0029), 'k': tensor(69285.8281)}\n",
      "(43, 44) : {'mean': tensor(0.1527), 'std': tensor(0.0034), 'k': tensor(52413.3438)}\n",
      "(44, 45) : {'mean': tensor(0.1351), 'std': tensor(0.0028), 'k': tensor(75165.9453)}\n",
      "(45, 46) : {'mean': tensor(0.1458), 'std': tensor(0.0030), 'k': tensor(68035.5000)}\n",
      "(46, 47) : {'mean': tensor(0.1527), 'std': tensor(0.0033), 'k': tensor(54138.6133)}\n",
      "(47, 48) : {'mean': tensor(0.1349), 'std': tensor(0.0028), 'k': tensor(75987.8750)}\n",
      "(48, 49) : {'mean': tensor(0.1450), 'std': tensor(0.0029), 'k': tensor(68691.6172)}\n",
      "(49, 50) : {'mean': tensor(0.1530), 'std': tensor(0.0034), 'k': tensor(52611.5352)}\n",
      "(50, 51) : {'mean': tensor(0.1352), 'std': tensor(0.0028), 'k': tensor(76414.6797)}\n",
      "(51, 52) : {'mean': tensor(0.1461), 'std': tensor(0.0030), 'k': tensor(67004.7812)}\n",
      "(52, 53) : {'mean': tensor(0.1504), 'std': tensor(0.0034), 'k': tensor(51936.2422)}\n",
      "(53, 54) : {'mean': tensor(0.1344), 'std': tensor(0.0028), 'k': tensor(74544.8438)}\n",
      "(54, 55) : {'mean': tensor(0.1454), 'std': tensor(0.0030), 'k': tensor(66512.3438)}\n",
      "(55, 56) : {'mean': tensor(0.1526), 'std': tensor(0.0034), 'k': tensor(51250.1680)}\n",
      "(56, 57) : {'mean': tensor(0.1347), 'std': tensor(0.0028), 'k': tensor(73855.1562)}\n",
      "(57, 58) : {'mean': tensor(0.1450), 'std': tensor(0.0030), 'k': tensor(66915.2812)}\n",
      "(58, 59) : {'mean': tensor(0.1551), 'std': tensor(0.0033), 'k': tensor(56011.5625)}\n",
      "Angles: \n",
      "(0, 1, 2) : {'mean': tensor(1.9699), 'std': tensor(0.0628), 'k': tensor(151.2267)}\n",
      "(1, 2, 3) : {'mean': tensor(2.0640), 'std': tensor(0.0485), 'k': tensor(253.5134)}\n",
      "(2, 3, 4) : {'mean': tensor(2.1658), 'std': tensor(0.0606), 'k': tensor(162.3382)}\n",
      "(3, 4, 5) : {'mean': tensor(1.9437), 'std': tensor(0.0728), 'k': tensor(112.5383)}\n",
      "(4, 5, 6) : {'mean': tensor(2.0502), 'std': tensor(0.0475), 'k': tensor(263.9885)}\n",
      "(5, 6, 7) : {'mean': tensor(2.1601), 'std': tensor(0.0558), 'k': tensor(191.3955)}\n",
      "(6, 7, 8) : {'mean': tensor(1.9477), 'std': tensor(0.0607), 'k': tensor(161.6919)}\n",
      "(7, 8, 9) : {'mean': tensor(2.0556), 'std': tensor(0.0460), 'k': tensor(281.1780)}\n",
      "(8, 9, 10) : {'mean': tensor(2.1559), 'std': tensor(0.0536), 'k': tensor(207.6854)}\n",
      "(9, 10, 11) : {'mean': tensor(1.9757), 'std': tensor(0.0524), 'k': tensor(217.1524)}\n",
      "(10, 11, 12) : {'mean': tensor(2.0528), 'std': tensor(0.0459), 'k': tensor(282.5728)}\n",
      "(11, 12, 13) : {'mean': tensor(2.1669), 'std': tensor(0.0545), 'k': tensor(200.9825)}\n",
      "(12, 13, 14) : {'mean': tensor(1.9780), 'std': tensor(0.0521), 'k': tensor(219.9250)}\n",
      "(13, 14, 15) : {'mean': tensor(2.0638), 'std': tensor(0.0465), 'k': tensor(276.0043)}\n",
      "(14, 15, 16) : {'mean': tensor(2.1552), 'std': tensor(0.0525), 'k': tensor(216.6688)}\n",
      "(15, 16, 17) : {'mean': tensor(1.9656), 'std': tensor(0.0510), 'k': tensor(228.7688)}\n",
      "(16, 17, 18) : {'mean': tensor(2.0487), 'std': tensor(0.0464), 'k': tensor(277.1868)}\n",
      "(17, 18, 19) : {'mean': tensor(2.1661), 'std': tensor(0.0532), 'k': tensor(210.9661)}\n",
      "(18, 19, 20) : {'mean': tensor(1.9716), 'std': tensor(0.0529), 'k': tensor(213.3445)}\n",
      "(19, 20, 21) : {'mean': tensor(2.0570), 'std': tensor(0.0464), 'k': tensor(276.7275)}\n",
      "(20, 21, 22) : {'mean': tensor(2.1639), 'std': tensor(0.0528), 'k': tensor(213.8324)}\n",
      "(21, 22, 23) : {'mean': tensor(1.9715), 'std': tensor(0.0515), 'k': tensor(224.9767)}\n",
      "(22, 23, 24) : {'mean': tensor(2.0470), 'std': tensor(0.0469), 'k': tensor(271.0843)}\n",
      "(23, 24, 25) : {'mean': tensor(2.1716), 'std': tensor(0.0539), 'k': tensor(205.4758)}\n",
      "(24, 25, 26) : {'mean': tensor(1.9759), 'std': tensor(0.0519), 'k': tensor(221.2774)}\n",
      "(25, 26, 27) : {'mean': tensor(2.0550), 'std': tensor(0.0468), 'k': tensor(271.9339)}\n",
      "(26, 27, 28) : {'mean': tensor(2.1625), 'std': tensor(0.0540), 'k': tensor(204.5282)}\n",
      "(27, 28, 29) : {'mean': tensor(1.9877), 'std': tensor(0.0534), 'k': tensor(208.6771)}\n",
      "(28, 29, 30) : {'mean': tensor(2.0638), 'std': tensor(0.0473), 'k': tensor(265.9169)}\n",
      "(29, 30, 31) : {'mean': tensor(2.1626), 'std': tensor(0.0543), 'k': tensor(202.1479)}\n",
      "(30, 31, 32) : {'mean': tensor(1.9575), 'std': tensor(0.0512), 'k': tensor(227.6797)}\n",
      "(31, 32, 33) : {'mean': tensor(2.0626), 'std': tensor(0.0464), 'k': tensor(276.9755)}\n",
      "(32, 33, 34) : {'mean': tensor(2.1546), 'std': tensor(0.0528), 'k': tensor(213.4683)}\n",
      "(33, 34, 35) : {'mean': tensor(1.9801), 'std': tensor(0.0517), 'k': tensor(222.6385)}\n",
      "(34, 35, 36) : {'mean': tensor(2.0638), 'std': tensor(0.0465), 'k': tensor(275.6647)}\n",
      "(35, 36, 37) : {'mean': tensor(2.1643), 'std': tensor(0.0542), 'k': tensor(203.1789)}\n",
      "(36, 37, 38) : {'mean': tensor(1.9751), 'std': tensor(0.0528), 'k': tensor(213.6249)}\n",
      "(37, 38, 39) : {'mean': tensor(2.0551), 'std': tensor(0.0470), 'k': tensor(270.0779)}\n",
      "(38, 39, 40) : {'mean': tensor(2.1812), 'std': tensor(0.0532), 'k': tensor(210.6344)}\n",
      "(39, 40, 41) : {'mean': tensor(1.9524), 'std': tensor(0.0507), 'k': tensor(232.2336)}\n",
      "(40, 41, 42) : {'mean': tensor(2.0569), 'std': tensor(0.0462), 'k': tensor(279.1018)}\n",
      "(41, 42, 43) : {'mean': tensor(2.1646), 'std': tensor(0.0542), 'k': tensor(203.1245)}\n",
      "(42, 43, 44) : {'mean': tensor(1.9851), 'std': tensor(0.0527), 'k': tensor(214.2916)}\n",
      "(43, 44, 45) : {'mean': tensor(2.0599), 'std': tensor(0.0467), 'k': tensor(273.6218)}\n",
      "(44, 45, 46) : {'mean': tensor(2.1705), 'std': tensor(0.0545), 'k': tensor(200.3458)}\n",
      "(45, 46, 47) : {'mean': tensor(1.9768), 'std': tensor(0.0520), 'k': tensor(220.7615)}\n",
      "(46, 47, 48) : {'mean': tensor(2.0495), 'std': tensor(0.0468), 'k': tensor(272.6782)}\n",
      "(47, 48, 49) : {'mean': tensor(2.1651), 'std': tensor(0.0560), 'k': tensor(190.3380)}\n",
      "(48, 49, 50) : {'mean': tensor(1.9783), 'std': tensor(0.0536), 'k': tensor(207.1741)}\n",
      "(49, 50, 51) : {'mean': tensor(2.0576), 'std': tensor(0.0470), 'k': tensor(270.2874)}\n",
      "(50, 51, 52) : {'mean': tensor(2.1589), 'std': tensor(0.0575), 'k': tensor(180.0182)}\n",
      "(51, 52, 53) : {'mean': tensor(1.9876), 'std': tensor(0.0578), 'k': tensor(178.7193)}\n",
      "(52, 53, 54) : {'mean': tensor(2.0471), 'std': tensor(0.0478), 'k': tensor(260.7987)}\n",
      "(53, 54, 55) : {'mean': tensor(2.1629), 'std': tensor(0.0549), 'k': tensor(197.7287)}\n",
      "(54, 55, 56) : {'mean': tensor(1.9510), 'std': tensor(0.0552), 'k': tensor(195.9822)}\n",
      "(55, 56, 57) : {'mean': tensor(2.0525), 'std': tensor(0.0468), 'k': tensor(271.6356)}\n",
      "(56, 57, 58) : {'mean': tensor(2.1685), 'std': tensor(0.0572), 'k': tensor(182.0366)}\n",
      "(57, 58, 59) : {'mean': tensor(1.9397), 'std': tensor(0.0634), 'k': tensor(148.2772)}\n"
     ]
    }
   ],
   "source": [
    "stats = GeometryStatistics(coords[::100], backbone_inds='all',\n",
    "                           get_all_distances=True,\n",
    "                           get_backbone_angles=True,\n",
    "                           get_backbone_dihedrals=True)\n",
    "\n",
    "bond_list, bond_keys = stats.get_prior_statistics(features='Bonds', as_list=True)\n",
    "bond_indices = stats.return_indices('Bonds')\n",
    "\n",
    "angle_list, angle_keys = stats.get_prior_statistics(features='Angles', as_list=True)\n",
    "angle_indices = stats.return_indices('Angles')\n",
    "print(\"We have {} backbone beads, {} bonds, and {} angles.\".format(\n",
    "                        coords.shape[1], len(bond_list), len(angle_list)))\n",
    "print(\"Bonds: \")\n",
    "for key, stat in zip(bond_keys, bond_list):\n",
    "    print(\"{} : {}\".format(key, stat))\n",
    "print(\"Angles: \")\n",
    "for key, stat in zip(angle_keys, angle_list):\n",
    "    print(\"{} : {}\".format(key, stat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the harmonic constants for the bonds and angles, we want the zscores for every feature, including the dihedrals and the non-bond (i.e., non-adjacent) pairwise distances. Such zscore normalization is useful in speeding up convergence of CGnet parameters during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2 statistics for 1942 features.\n"
     ]
    }
   ],
   "source": [
    "all_stats, _ = stats.get_prior_statistics(as_list=True)\n",
    "num_feats = len(all_stats)\n",
    "zscores, _ = stats.get_zscore_array()\n",
    "print(\"We have {} statistics for {} features.\".format(zscores.shape[0], zscores.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Next, we determine hyperparameters. Following the original paper [1], we use:\n",
    "\n",
    "* 5 layers with 160 nodes each and tanh activation,\n",
    "* random sampling from the dataset with a batch size of 512,\n",
    "* Adam optimization with an intial learning rate of 0.003\n",
    "* epochal multiplicative learning rate decay with decay constant $\\gamma=0.3$, and\n",
    "* L2 Lipschitz regularization via spectral normalization ($\\lambda=4.0$) of each weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "n_layers = 5\n",
    "n_nodes = 160\n",
    "activation = nn.Tanh()\n",
    "batch_size = 512\n",
    "learning_rate = 0.003\n",
    "rate_decay = 0.3\n",
    "lipschitz_strength = 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our network architecture and training\n",
    "\n",
    "Now we are ready to start designing the architecture of our CGnet. We follow the optimal architecture in the CGnet paper [1], which consists of:\n",
    "\n",
    "- a feature layer that outputs pairwise distances, angles, and dihedral angles from the Cartesian input trajectory that subtracts the means and divides by the standard deviations,\n",
    "- hidden linear layers (bias term inclusive) with the first three followed by nonlinear activation, and\n",
    "- harmonic prior layers that compute prior energies from bonds and angles.\n",
    "\n",
    "The layers are stored in the list `layers` and the priors are stored in the list `priors`.\n",
    "\n",
    "<img src=\"./figs/CGnet.png\" width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by scaling according to mean and standard deviation\n",
    "layers = [ZscoreLayer(zscores)]\n",
    "\n",
    "# The first hidden layer goes from number of features to 160\n",
    "num_feat = len(all_stats)\n",
    "layers += LinearLayer(num_feat, n_nodes, activation=activation)\n",
    "\n",
    "# The inner hidden layers stay the same size\n",
    "for _ in range(n_layers - 1):\n",
    "    layers += LinearLayer(n_nodes, n_nodes, activation=activation)\n",
    "\n",
    "# The last layer produces a single value\n",
    "layers += LinearLayer(n_nodes, 1, activation=None)\n",
    "\n",
    "# Construct prior energy layers\n",
    "priors  = [HarmonicLayer(bond_indices, bond_list)]\n",
    "priors += [HarmonicLayer(angle_indices, angle_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature layer, we create an instance of `GeometryFeature()`, which provides differentiable transformations from cartesian coordinate inputs into the CGnet to roto-translationally invariant internal features, such as angles, pairwise distances, and dihedrals (in fact, any set of general features can be passed to a `GeometryFeature()` initialization as a list of tuples of beads involved in the interaction).\n",
    "\n",
    "By default, `GeometryFeature()` featurizes cartesian inputs into pairwise distances, angles, dihedral cosines, and dihedral sines. To make sure that any statistics used for priors or other preprocessing remain consistent with the ouput of the feature layer, the `stats.feature_tuples` attribute can be passed as an argument to `GeometryFeature()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = GeometryFeature(feature_tuples=stats.feature_tuples, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our architecture, we are ready to build our CGnet. Because we are recovering the coarse grain potential by matching the potential of mean force (PMF) from the all-atom + solvent model of alanine dipeptide, we imbue our network with the `ForceLoss()` criterion. We can look at our network by printing the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGnet(\n",
      "  (arch): Sequential(\n",
      "    (0): ZscoreLayer()\n",
      "    (1): Linear(in_features=1942, out_features=160, bias=True)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=160, out_features=160, bias=True)\n",
      "    (4): Tanh()\n",
      "    (5): Linear(in_features=160, out_features=160, bias=True)\n",
      "    (6): Tanh()\n",
      "    (7): Linear(in_features=160, out_features=160, bias=True)\n",
      "    (8): Tanh()\n",
      "    (9): Linear(in_features=160, out_features=160, bias=True)\n",
      "    (10): Tanh()\n",
      "    (11): Linear(in_features=160, out_features=1, bias=True)\n",
      "  )\n",
      "  (priors): Sequential(\n",
      "    (0): HarmonicLayer()\n",
      "    (1): HarmonicLayer()\n",
      "  )\n",
      "  (criterion): ForceLoss()\n",
      "  (feature): GeometryFeature()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ala2_net = CGnet(layers, ForceLoss(),\n",
    "                 feature=feature_layer,\n",
    "                 priors=priors).to(device)\n",
    "print(ala2_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up our training tools. We will neglect cross-validation for brevity, but this is __not__ acceptable for real analysis. To do cross-validation, a `testloader` would be needed with validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training tools\n",
    "\n",
    "trainloader = DataLoader(ala_data, sampler=RandomSampler(ala_data),\n",
    "                         batch_size=batch_size)\n",
    "optimizer = torch.optim.Adam(ala2_net.parameters(),\n",
    "                             lr=learning_rate)\n",
    "def regularizer(model, strength=lipschitz_strength):\n",
    "        lipschitz_projection(model, strength=strength)\n",
    "scheduler = MultiStepLR(optimizer,milestones=[1,2,3,4,5],\n",
    "                        gamma=rate_decay)\n",
    "\n",
    "num_epochs = 100\n",
    "save_model = False\n",
    "directory = '.' # to save model\n",
    "\n",
    "epochal_train_losses = []\n",
    "epochal_test_losses  = []\n",
    "verbose = True\n",
    "batch_freq = 1000\n",
    "epoch_freq = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!\n",
    "\n",
    "Now we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1000, Loss: 9870.52\n",
      "Batch: 1000, Loss: 578.54\n",
      "Batch: 1000, Loss: 270.11\n",
      "Batch: 1000, Loss: 181.79\n",
      "Batch: 1000, Loss: 155.51\n",
      "Batch: 1000, Loss: 132.44\n",
      "Batch: 1000, Loss: 127.01\n",
      "Batch: 1000, Loss: 118.89\n",
      "Batch: 1000, Loss: 119.22\n",
      "Batch: 1000, Loss: 111.59\n",
      "Batch: 1000, Loss: 104.07\n",
      "Batch: 1000, Loss: 102.62\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    test_loss = 0.00\n",
    "    train_loss = dataset_loss(ala2_net, trainloader,\n",
    "                              optimizer, regularizer,\n",
    "                              verbose_interval=batch_freq)\n",
    "        \n",
    "    # test_loss = dataset_loss(ala2_net, train_loader)\n",
    "\n",
    "    if verbose:\n",
    "        if epoch % epoch_freq == 0:\n",
    "            print(\n",
    "                 \"Epoch: {} | Train loss: {:.2f} | Test loss: {:.2f}\\n\".format(\n",
    "                  epoch, train_loss, test_loss))\n",
    "        epochal_train_losses.append(train_loss)\n",
    "        #epochal_test_losses.append(test_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "        \n",
    "if save_model:\n",
    "    torch.save(ala2_net,\"{}/ala2_net.pt\".format(directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot at the training loss over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(0,len(epochal_train_losses),1),\n",
    "         epochal_train_losses, label='Training Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(np.arange(1,5))\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the coarse-grained force field to simulate system dynamics\n",
    "\n",
    "First we set up initial coordinates which will be used as the starting points for independent simulations.\n",
    "\n",
    "We will run 1000 independent simulations seeded evenly the original dataset for 1000 timesteps each. These paramters can be changed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sims = 1000\n",
    "n_timesteps = 1000\n",
    "save_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_coords = np.concatenate([coords[i].reshape(-1,5,3)\n",
    "                                 for i in np.arange(0, 10000, 10000//n_sims)],\n",
    "                                 axis=0)\n",
    "initial_coords = torch.tensor(initial_coords, requires_grad=True).to(device)\n",
    "print(\"Produced {} initial coordinates.\".format(len(initial_coords)))\n",
    "\n",
    "sim = Simulation(ala2_net, initial_coords, length=n_timesteps,\n",
    "                 save_interval=save_interval, beta=stats.beta,\n",
    "                 save_potential=True, device=device,\n",
    "                 log_interval=100, log_type='print')\n",
    "\n",
    "traj = sim.simulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the simulation output\n",
    "\n",
    "Now we want to see what our simulated coarse-grained dynamics are like. First we create an `mdtraj`-friendly [2] `CGMolecule` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['C', 'N', 'CA', 'C', 'N']\n",
    "resseq = [1, 2, 2, 2, 3]\n",
    "resmap = {1: 'ACE', 2: 'ALA', 3: 'NME'}\n",
    "\n",
    "ala2_cg = CGMolecule(names=names, resseq=resseq, resmap=resmap,\n",
    "                          bonds='standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can input our training and simulated coordinates in order to make `mdtraj` trajectories out of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ala2_traj = ala2_cg.make_trajectory(coords)\n",
    "ala2_simulated_traj = ala2_cg.make_trajectory(np.concatenate(traj, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `mdtraj`, we compute the backbone $\\phi$ and $\\psi$ torsional angles of our reference and simulated systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, phi = md.compute_phi(ala2_traj)\n",
    "_, psi = md.compute_psi(ala2_traj)\n",
    "\n",
    "_, sim_phi = md.compute_phi(ala2_simulated_traj)\n",
    "_, sim_psi = md.compute_psi(ala2_simulated_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reference coordinates, we compute the potential using our network. For the simulated coordinates, we saved the potential when we ran the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot, _ = ala2_net.forward(torch.tensor(coords, requires_grad=True))\n",
    "pot = pot.detach().numpy()\n",
    "pot = pot - np.min(pot)\n",
    "\n",
    "sim_pot = np.concatenate(sim.simulated_potential, axis=0)\n",
    "sim_pot = sim_pot - np.min(sim_pot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the reference and simulation Ramachandran plots. Since we didn't train for very long, and because we used only a small amount of data, this shouldn't be too impressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(8,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(phi.reshape(-1), psi.reshape(-1),\n",
    "            c=pot.flatten(),\n",
    "            cmap=plt.get_cmap(\"viridis\"),alpha=0.5,s=0.5)\n",
    "plt.xlabel(r'$\\phi$',fontsize=16)\n",
    "plt.ylabel(r'$\\psi$',fontsize=16)\n",
    "plt.xlim(-np.pi, np.pi)\n",
    "plt.ylim(-np.pi, np.pi)\n",
    "plt.title(r'Original all-atom trajectory')\n",
    "clb=plt.colorbar()\n",
    "clb.ax.set_title(r'$U\\left(\\frac{kcal}{mol}\\right)$')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(sim_phi.reshape(-1), sim_psi.reshape(-1),\n",
    "            c=sim_pot.flatten(),\n",
    "            cmap=plt.get_cmap(\"viridis\"),alpha=0.5,s=0.5)\n",
    "plt.xlabel(r'$\\phi$',fontsize=16)\n",
    "plt.ylabel(r'$\\psi$',fontsize=16)\n",
    "plt.xlim(-np.pi, np.pi)\n",
    "plt.ylim(-np.pi, np.pi)\n",
    "plt.title('Simulated CG trajectory')\n",
    "clb=plt.colorbar()\n",
    "clb.ax.set_title(r'$U\\left(\\frac{kcal}{mol}\\right)$')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the simulated free energy $F = U - TS$. To do this, we histogram the 2-dimensional dihedral trajectory and convert counts to populations $\\pi_i$ for each bin $i$. Then, we transform the nonzero populations into energies according to $F_i = \\beta^{-1}\\log\\pi_i$.\n",
    "\n",
    "First we set up a graphing utility for our Ramachandran plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ramachandran(phi, psi, bins=60, cmap=plt.cm.magma):\n",
    "    edges = np.array([[-np.pi, np.pi], [-np.pi, np.pi]])\n",
    "    counts, _, _ = np.histogram2d(psi.reshape(-1),\n",
    "                                  phi.reshape(-1),\n",
    "                                  bins=bins,\n",
    "                                  range=edges)\n",
    "    populations = counts / np.sum(counts)\n",
    "    \n",
    "    # compute energies for only non-zero entries\n",
    "    # 1/beta is approximately 0.6 kcal/mol at 300 K\n",
    "    energies = -0.6*np.log(populations,\n",
    "                           out=np.zeros_like(populations),\n",
    "                           where=(populations > 0))\n",
    "    \n",
    "    # make the lowest energy slightly above zero\n",
    "    energies = np.where(energies,\n",
    "                        energies-np.min(energies[np.nonzero(energies)]) + 1e-6,\n",
    "                        0)\n",
    "    \n",
    "    # mask the zero values from the colormap\n",
    "    zvals_masked = np.ma.masked_where(energies == 0, energies)\n",
    "\n",
    "    cmap.set_bad(color='white')\n",
    "    img = plt.imshow(zvals_masked, interpolation='nearest', cmap = cmap)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.xticks([-0.5, bins/2, bins], \n",
    "               [r'$-\\pi$', r'$0$', r'$\\pi$'])\n",
    "\n",
    "    plt.yticks([-0.5, bins/2, bins],\n",
    "               [r'$-\\pi$', r'$0$', r'$\\pi$'])\n",
    "    \n",
    "    plt.xlabel(r'$\\phi$',fontsize=16)\n",
    "    plt.ylabel(r'$\\psi$',fontsize=16)\n",
    "    \n",
    "    cb=plt.colorbar()\n",
    "    cb.ax.set_title(r'$\\tilde{F}\\left(\\frac{kcal}{mol}\\right)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the free energies for the original and CG trajectories. Again, we expect this to look bad because we used only 1% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(8,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plot_ramachandran(phi, psi)\n",
    "plt.title('Original all-atom trajectory')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_ramachandran(sim_phi, sim_psi)\n",
    "plt.title('Simulated CG trajectory')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *References*\n",
    "\n",
    "[1] Wang, J., Olsson, S., Wehmeyer, C., Pérez, A., Charron, N. E., de Fabritiis, G., Noé, F., and Clementi, C. (2019). Machine Learning of Coarse-Grained Molecular Dynamics Force Fields. _ACS Central Science._ https://doi.org/10.1021/acscentsci.8b00913\n",
    "\n",
    "[2] McGibbon, R. T., Beauchamp, K. A., Harrigan, M. P., Klein, C., Swails, J. M., Hernández, C. X., Schwantes, C. R., Wang, L.-P., Lane, T. J., and Pande, V. S. (2015). MDTraj: A Modern Open Library for the Analysis of Molecular Dynamics Trajectories. _Biophys J._ http://dx.doi.org/10.1016/j.bpj.2015.08.015"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
